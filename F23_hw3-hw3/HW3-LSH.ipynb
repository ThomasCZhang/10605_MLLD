{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a60106fb",
   "metadata": {},
   "source": [
    "# CMU notebook\n",
    "Before you turn these assignments in, make sure everything runs as expected. Be sure to click **run all** (in the upper right).\n",
    "\n",
    "Follow the guide and fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc679103",
   "metadata": {},
   "source": [
    "# CMU Machine Learning with Large Datasets\n",
    "\n",
    "## Homework 3 - Coding 1: LSH for Image Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c24542f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T06:17:11.639160Z",
     "start_time": "2023-02-01T06:17:11.636304Z"
    }
   },
   "outputs": [],
   "source": [
    "# Who did you collaborate with on this assignment? \n",
    "# if no one, collaborators should contain an empty string,\n",
    "# else list your collaborators below\n",
    "\n",
    "collaborators = [\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443b22a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T06:17:11.937161Z",
     "start_time": "2023-02-01T06:17:11.934241Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    collaborators\n",
    "except:\n",
    "    raise AssertionError(\"you did not list your collaborators, if any\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc08eb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOU CAN MOST LIKELY IGNORE THIS CELL. This is only of use for running this notebook locally.\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "sc = pyspark.SparkContext(appName=\"hw\")\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "print(\"spark context started\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9003facc",
   "metadata": {},
   "source": [
    "# **Approximate Nearest Neighbors Search and LSH**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ff455e",
   "metadata": {},
   "source": [
    "Nearest neighbor (NN) search is an important concept in Machine Learning as it arises in numerous applications such as reverse image search, semantic search, or genome sequencing. Formally, given a set of points $X\\subset\\mathbb{R}^d$ and a query point $q\\in \\mathbb{R}^d$, the goal of NN search is to find the closest point(s) in $X$ to $q$. Defining \"nearest\" is perhaps the most important aspect of a NN algorithm, however, in practice we tend to use euclidean distance. This can be problematic in very high-dimensional spaces, which is why typically the data is first reduced via PCA or other feature representation models such as deep neural networks.\n",
    "\n",
    "Another challenge of the standard, exact NN algorithm is its runtime complexity. For a single query point, NN has a running time of $O(|X|\\cdot d)$. In practice, we commonly compute a nearest neighbors graph which requires computing all pairs of distances for a total running time of $O(|X|^2\\cdot d)$, which can be prohibitive for very large datasets.\n",
    "\n",
    "For this reason, approximate nearest neighbor (ANN) algorithms have found an increase in popularity. The goal of an ANN algorithm is to return neighbors whose distance from the query point is no larger than some constant $c$ times the distance from the true nearest points, while consuming less memory and making faster inferences.\n",
    "\n",
    "In this homework, we will explore a variant of ANN search for images based on locality-sensitive hashing (LSH). As discussed in class, LSH consists of a hashing function which encourages collisions between nearby points in the original space. Choosing the right hashing function is somewhat of an art and depends largely on the domain. In this assignment, we will develop the **cross-polytope hash** ([ref](https://people.csail.mit.edu/ludwigs/papers/nips15_crosspolytopelsh.pdf)) for image data. Roughly, the hash first projects an input sample to a lower dimensional space (by using a JL style projection) and then selects the coordinate with the highest magnitude in the resulting projected vector. The *index* of this coordinate, along with its sign, serves as a hash for the given input. One could then perform multiple such projections and stack all the individual hash values to form a *fingerprint*. The number of projections and their dimensionality is a hyperparameter that we need to tune. Increasing the number of dimensions tends to lead to fewer collisions.\n",
    "\n",
    "This variant of LSH is fast (runtime is dominated by the projection step) and easy to implement. Our goal is to explore its performance on images.\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "* **0. Preliminaries**\n",
    " * **0.1 Load the Data**\n",
    " \n",
    "* **1. Data Exploration**\n",
    " * **1.1 Display Random Images**\n",
    " * **1.2 Compute Statistics about the Data**\n",
    "* **2. Implementing LSH**\n",
    " * **2.1 Write Code to Generate JL style maps**\n",
    " * **2.2 Implement the Hashing Function**\n",
    " * **2.3 Implement the Fingerprint Function**\n",
    "* **3. Processing**\n",
    " * **3.1 Compute Fingerprints for the Given Dataset**\n",
    " * **3.2 Distribute Images into Buckets**\n",
    " * **3.3 Plot Bucket Sizes**\n",
    " * **3.4 Filter Buckets with at least Two Elements**\n",
    "* **4. Analysis and Visualization**\n",
    " * **4.1 Write Code to Map a Query Image to its Neighbors**\n",
    " * **4.2 Visualize LSH Neighbors**\n",
    "* **5. Feature Extraction Using a Pre-Trained VGG**\n",
    " * **5.1 Compute Neighbors based on VGG Embeddings**\n",
    " * **5.2 Compare VGG Neighbors to LSH Neighbors**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d097c66",
   "metadata": {},
   "source": [
    "Some links:\n",
    "\n",
    "- **RDD API**: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.html\n",
    "- **Cross-polytope** hash: https://people.csail.mit.edu/ludwigs/papers/nips15_crosspolytopelsh.pdf\n",
    "- **Caltech 101 dataset**: https://www.tensorflow.org/datasets/catalog/caltech101\n",
    "- **The S3 bucket containing the data**: https://s3.console.aws.amazon.com/s3/buckets/10605-f23-hw3-data?region=us-east-1&prefix=101_ObjectCategories/&showversions=false\n",
    "\n",
    "This homework can be solved by using RDD function calls only, but if you need to, you can also use DataFrames\n",
    "- **DataFrame API** https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8830a354",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T06:04:49.278862Z",
     "start_time": "2023-02-01T06:04:49.059424Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import pickle\n",
    "from itertools import islice, zip_longest\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from nose.tools import assert_equal, assert_true\n",
    "from PIL import Image\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e821ec",
   "metadata": {},
   "source": [
    "## Part 0. Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe62d68",
   "metadata": {},
   "source": [
    "For this assignment, we will be using the [Caltech-101 dataset](https://www.tensorflow.org/datasets/catalog/caltech101). Caltech-101 contains 102 (colored) image classes, where one of them is a background clutter class. The images are of varying sizes, so we need to resize them to the same shape first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb8ee3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SHAPE = (64, 64, 3)  # shape used for plotting figures\n",
    "FP_SHAPE = (8, 8, 3)  # shape used for computing the fingerprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32595bc5",
   "metadata": {},
   "source": [
    "### 0.1 Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11352dba",
   "metadata": {},
   "source": [
    "We provide a few helper functions that preprocess each image and return a Row containing *label*, *filename*, and two arrays with image data. *image* is the array we will use to perform our projections and compute the fingerprints, and the other, larger array *image_lg* will be used for plotting only.\n",
    "\n",
    "We perform the following preprocessing steps:\n",
    "\n",
    "1. Convert binary data to a numpy array containing pixel values.\n",
    "2. Convert to RGB in case the image is grayscale or contains an extra alpha channel.\n",
    "3. Resize the image.\n",
    "4. Convert from BGR to RGB.\n",
    "5. Flatten the image vectors.\n",
    "6. Add an index to each row as an identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa74f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doc: https://pillow.readthedocs.io/en/stable/handbook/concepts.html#concept-modes\n",
    "channels_to_mode = {\n",
    "    1: 'L',\n",
    "    3: 'RGB',\n",
    "    4: 'RGBA',\n",
    "}\n",
    "\n",
    "def bgr_to_rgb(img):\n",
    "    \"\"\"The image datasource built in spark uses opencv to load images\n",
    "    and due to historical reasons, opencv loads the images in BGR format.\n",
    "    Most other libraries, such as pillow or matplotlib assume an RGB format\n",
    "    so we have to convert the format ourselves.\n",
    "    \n",
    "    Source: https://spark.apache.org/docs/latest/ml-datasource.html#image-data-source\n",
    "    \"\"\"\n",
    "    B, G, R = img.T\n",
    "    return np.array((R, G, B)).T\n",
    "\n",
    "def resize_img(row):\n",
    "    \"\"\"Spark loads the images as a bytearray, so we need to convert it to a\n",
    "    numpy array. Also, some images may contain an additional alpha channel,\n",
    "    which we are not interested in, so we remove it. Finally we resize the image\n",
    "    and convert the format from BGR to RGB.\n",
    "    \"\"\"\n",
    "    mode = channels_to_mode[row.nChannels]\n",
    "    # frombytes expects a bytes-like object not a bytearray, so we need to call `bytes`.\n",
    "    img = Image.frombytes(mode=mode, data=bytes(row.data), size=(row.width, row.height))\n",
    "    img = img.convert('RGB') if (mode != 'RGB') else img\n",
    "    # We keep one high-res image for plotting\n",
    "    # and a smaller version for LSH.\n",
    "    img_lg = img.resize(SHAPE[:2])  # don't need channels for this func call\n",
    "    img_fp = img.resize(FP_SHAPE[:2])\n",
    "    img_arr = bgr_to_rgb(np.asarray(img_lg))\n",
    "    img_arr_fp = bgr_to_rgb(np.asarray(img_fp))\n",
    "    return img_arr.flatten(), img_arr_fp.flatten()  # flatten the images\n",
    "\n",
    "def bytes_row_to_array_row(row):\n",
    "    # Each row in the inner RDD is a Row(image=Row(data...)),\n",
    "    # so we get rid of one unneccessary layer by mapping to row.image.\n",
    "    row = row.image\n",
    "    image_lg, image = resize_img(row)\n",
    "    return Row(\n",
    "        label=row.origin.rsplit('/', maxsplit=2)[-2],  # e.g., anchor\n",
    "        filename='/'.join(row.origin.rsplit('/', maxsplit=2)[-2:]),  # e.g., anchor/image_0001.jpg\n",
    "        image=image,\n",
    "        image_lg=image_lg,\n",
    "    )\n",
    "\n",
    "def add_index_to_row(image_rdd):\n",
    "    \"\"\"When working with kNN, it is typically better to return\n",
    "    indices of the neighbors rather than the full images themselves\n",
    "    for efficiency. Here we add a unique index to each row.\n",
    "    \"\"\"\n",
    "    rdd_with_zipped_idx = image_rdd.zipWithIndex()\n",
    "    # This rdd consists of tuples in the format (row, idx).\n",
    "    # For convenience, we convert to row(idx, data...).\n",
    "    return rdd_with_zipped_idx.map(lambda x: Row(\n",
    "        idx=x[1],\n",
    "        **x[0].asDict(),\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217d34a0",
   "metadata": {},
   "source": [
    "The S3 bucket containing all the image files can be accessed here: https://s3.console.aws.amazon.com/s3/buckets/10605-f23-hw3-data?region=us-east-1&prefix=101_ObjectCategories/&showversions=false.\n",
    "This is a public bucket.\n",
    "\n",
    "While setting up the EMR cluster we added a \"Step\" that copies the data from this bucket to your Hadoop filesystem (core nodes) under `/data`. **Make sure that your step finishes running before executing the following cell.** Go to your EMR console and click on `Steps`. The status of your step should be marked as `Completed`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a29ee1",
   "metadata": {},
   "source": [
    "**Hint: Use a smaller data subset during development.** The spark `load` function supports pattern matching to read specific files from a directory.\n",
    "Note that some cells (towards the end) will test your code on specific images which may or may not be in your sample. In the latter case, an exception will be raised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211fa651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the data was moved to the hadoop filesystem,\n",
    "# we can access it directly using spark.\n",
    "image_rdd = spark.read.format(\"image\").load(\"/data/*/*.jpg\").rdd\n",
    "\n",
    "# You can instead use a smaller subset of the data during development.\n",
    "# E.g., pick only classes that start with an 'h'.\n",
    "\n",
    "# image_rdd = spark.read.format(\"image\").load(\"/data/h*/*.jpg\").rdd\n",
    "\n",
    "image_rdd = image_rdd.map(bytes_row_to_array_row)\n",
    "image_rdd = add_index_to_row(image_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b0016d",
   "metadata": {},
   "source": [
    "## Part 1. Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0fa475",
   "metadata": {},
   "source": [
    "### 1.1 Display Random Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd712fd",
   "metadata": {},
   "source": [
    "Let's first count the number of images in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0899572",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T06:05:11.782626Z",
     "start_time": "2023-02-01T06:05:05.546432Z"
    }
   },
   "outputs": [],
   "source": [
    "# Count the number of images\n",
    "n_images = image_rdd.count()\n",
    "print(f\"Found {n_images} images.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a686c78b",
   "metadata": {},
   "source": [
    "Now we take a random sample of five images and display them to make sure we loaded them properly. Since we flattened the image vectors earlier, we need to reshape back to (W, H, 3). You will need the `%matplot plt` magic command if running on AWS notebooks. Put this *at the end* of the cell. We also print the Row attributes for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ea3fc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T06:05:19.671190Z",
     "start_time": "2023-02-01T06:05:12.853979Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Take a random sample of 5 images\n",
    "example_rows = image_rdd.takeSample(withReplacement=False, num=5)\n",
    "# Print the Row columns\n",
    "print(f\"Row attributes: {example_rows[0].__fields__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79d46e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T06:05:21.108753Z",
     "start_time": "2023-02-01T06:05:20.868892Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot five images\n",
    "fig, axes = plt.subplots(ncols=5, figsize=(15, 5))\n",
    "for row, ax in zip(example_rows, axes.flat):\n",
    "    ax.imshow(np.asarray(row.image_lg).reshape(SHAPE))\n",
    "    ax.set_title(f\"label={row.label}, idx={row.idx}\")\n",
    "    ax.set_axis_off()\n",
    "plt.show()\n",
    "\n",
    "%matplot plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842bb35d",
   "metadata": {},
   "source": [
    "### 1.2 Compute Statistics about the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc7920a",
   "metadata": {},
   "source": [
    "Your first task is to compute the number of images under each class (label). It is always a good idea to plot this distribution to see any class imbalances in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb551816",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T06:05:31.113665Z",
     "start_time": "2023-02-01T06:05:27.576625Z"
    }
   },
   "outputs": [],
   "source": [
    "def countLabels(image_rdd):\n",
    "    \"\"\"Count the occurrences of each label.\n",
    "    \n",
    "    Parameters\n",
    "    __________\n",
    "    image_rdd: RDD\n",
    "        The image RDD we constructed above.\n",
    "    \n",
    "    Returns\n",
    "    _______\n",
    "    A dictonary mapping each label to an integer,\n",
    "    the number of images with that label.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "label_counts = countLabels(image_rdd)\n",
    "print(f\"Found {len(label_counts)} classes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2cba9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T06:05:33.803660Z",
     "start_time": "2023-02-01T06:05:32.848757Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 4))\n",
    "ax.bar(\n",
    "    range(len(label_counts)),\n",
    "    list(label_counts.values()),\n",
    "    tick_label=list(label_counts.keys())\n",
    ")\n",
    "ax.tick_params('x', rotation=90)\n",
    "ax.set_xlabel('Label')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Number of instances per class')\n",
    "plt.show()\n",
    "\n",
    "%matplot plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19719f52",
   "metadata": {},
   "source": [
    "## Part 2. Implementing LSH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d532dc",
   "metadata": {},
   "source": [
    "Now let us describe the cross-polytope hash in more details.\n",
    "\n",
    "Let $x\\in\\mathbb{R}^n$ be an n-dimensional vector (e.g., flattened image array). Let us also assume that we have a function $f: \\mathbb{R}^n \\to \\mathbb{R}^d$ that maps $x$ to a new coordinate system (i.e., a lower dimensional representation) such that distances between points are (approximately) preserved. Denote $y = f(x)$ and let $i$ be the index corresponding the the coordinate of $y$ with the largest magnitude, i.e., $|y_i| \\geq |y_j|, \\forall j\\neq i$. The hash of $x$ under the map $f$ is the pair $(i, \\text{sign}(y_i))$. We can denote $\\text{sign}(y_i)$ by `True` if it is non-negative, and `False` otherwise.\n",
    "\n",
    "In the paper linked above, the authors find the closest point to $y$ from $\\{\\pm e_i\\}_{1\\leq i\\leq d}$, where $e_i$ is the $i$-th standard basis vector of $\\mathbb{R}^d$ and use that as a hash of $x$. But note that the two formulations are equivalent if $y$ has unit length.\n",
    "\n",
    "We can pick $f$ to be a Johnson-Lindenstrauss style linear map. In this assignment, to construct such a map we sample from a standard normal $A_{ij}\\sim\\mathcal{N}(0, 1)$ and scale $A$ by the inverse of the square root of the dimension.\n",
    "\n",
    "Finally, note that we can construct multiple projections and compute a hash for each one. This should (hopefully) lead to higher quality neighbors. We collect all these hashes into a single list and call this list the **fingerprint** of $x$.\n",
    "\n",
    "**Example**\n",
    "\n",
    "Let $y_1=[1, 2, -3]$ and $y_2=[5, 2, -1]$. Then $h_1=(2,\\ \\text{False})$ and $h_2=(0,\\ \\text{True})$. Finally, the fingerprint is $((2,\\ \\text{False}),\\ (0,\\ \\text{True}))$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd9e4b5",
   "metadata": {},
   "source": [
    "### 2.1 Write Code to Generate JL style maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18704091",
   "metadata": {},
   "source": [
    "In the following function, `obj_dims` is a list containing the objective dimensions. These dimensions can take different values.\n",
    "\n",
    "You can use `np.random.normal` to sample from a standard normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6766b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T06:05:35.352278Z",
     "start_time": "2023-02-01T06:05:35.347723Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_random_linear_maps(n_dims, obj_dims):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    __________\n",
    "    n_dims: int\n",
    "        Original number of dimensions in the data.\n",
    "    dims: List[int]\n",
    "        List of integers corresponding to the\n",
    "        objective dimensions of the random vectors.\n",
    "        \n",
    "    Returns\n",
    "    _______\n",
    "    random_maps: List[np.ndarray]\n",
    "        List of random linear maps such that\n",
    "        random_maps[i].shape[1] == dims[i]. Each matrix row\n",
    "        should be scaled. In other words, scale each matrix\n",
    "        of column dimension d by 1/sqrt(d).\n",
    "    \"\"\"    \n",
    "    for obj_dim in obj_dims:\n",
    "        assert obj_dim < n_dims\n",
    "\n",
    "    random_maps = []\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return random_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e81352b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T06:05:37.016542Z",
     "start_time": "2023-02-01T06:05:37.012650Z"
    }
   },
   "outputs": [],
   "source": [
    "_random_maps = get_random_linear_maps(11, [3, 7])\n",
    "# Check that the correct number of matrices is returned\n",
    "assert_equal(len(_random_maps), 2)\n",
    "# Check that the shapes of the random matrices are correct\n",
    "assert_equal(_random_maps[0].shape, (11, 3))\n",
    "assert_equal(_random_maps[1].shape, (11, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57933ef2",
   "metadata": {},
   "source": [
    "### 2.2 Implement the Hashing Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5324966",
   "metadata": {},
   "source": [
    "The following function takes a single projection and compute the hash as explained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58998f61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T06:05:37.539907Z",
     "start_time": "2023-02-01T06:05:37.536369Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_largest_coordinate(v):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    __________\n",
    "    v: 1D list or array\n",
    "        The input vector\n",
    "        \n",
    "    Returns\n",
    "    _______\n",
    "    A tuple (int, bool)\n",
    "        where the int corresponds to the index of the coordinate with the\n",
    "        largest magnitude, and the bool is True if that coordinate is >= 0\n",
    "        and False otherwise.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce951dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T06:05:37.691334Z",
     "start_time": "2023-02-01T06:05:37.687159Z"
    }
   },
   "outputs": [],
   "source": [
    "_v = [-1, -2, 8, 5, -4]\n",
    "# Check that we are returning int's and bool's\n",
    "assert_true(isinstance(get_largest_coordinate(_v)[0], int))\n",
    "assert_true(isinstance(get_largest_coordinate(_v)[1], bool))\n",
    "# Check that the correct index is returned for positive coordinate\n",
    "assert_equal(get_largest_coordinate(_v), (2, True))\n",
    "_w = [-1, -2, 0, 5, -7]\n",
    "# Check that the correct index is returned for negative coordinate\n",
    "assert_equal(get_largest_coordinate(_w), (4, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f81d59",
   "metadata": {},
   "source": [
    "### 2.3 Implement the Fingerprint Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd0a249",
   "metadata": {},
   "source": [
    "The following function takes a (flattened) image vector and a list of random linear projections which are used to compute the fingerprint for that input. You can call the previously defined function `get_largest_coordinate`. Make sure to return a tuple of tuples, since lists are not hashable in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb900a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T06:05:38.665832Z",
     "start_time": "2023-02-01T06:05:38.662115Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_fingerprint(image, random_maps):\n",
    "    \"\"\"\n",
    "    NOTE: It is important that we don't return a list\n",
    "    but a tuple of tuples (or any other immutable\n",
    "    data structure, so that we can hash it).\n",
    "    \n",
    "    Parameters\n",
    "    __________\n",
    "    image: list\n",
    "        Flattened image data\n",
    "    random_maps: list of arrays\n",
    "    \n",
    "    Returns\n",
    "    _______\n",
    "    A tuple of tuples ((int, bool), ...)\n",
    "        corresponding to the largest coordinate index and sign\n",
    "        of each random projection.\n",
    "        You can use the `tuple(f for k in c)` syntax.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d708991",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T06:05:38.776639Z",
     "start_time": "2023-02-01T06:05:38.773097Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check that fingerprints are computed properly\n",
    "_image = [2, -4, -5]\n",
    "_map = [[1, 0, -1], [2, 1, -1], [0, 0, 1]]\n",
    "assert_equal(((0, False),), get_fingerprint(_image, [_map]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c185490",
   "metadata": {},
   "source": [
    "The following function should map each image to its fingerprint. We do not need to carry the original image data around. Make sure to return an RDD of Rows with the fingerprint column name set to `fp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322c6d37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T06:05:39.562567Z",
     "start_time": "2023-02-01T06:05:39.558706Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_rdd_with_fingerprints(image_rdd, random_maps):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    __________\n",
    "    rdd: image_rdd\n",
    "        Image data. Each row must contain an 'image' attribute.\n",
    "    random_maps: list\n",
    "        A list of arrays, each representing a random linear map.\n",
    "    \n",
    "    Returns\n",
    "    _______\n",
    "    rdd_with_hash: RDD\n",
    "        Image data with an extra column added corresponding to\n",
    "        the hash value (fingerprint). The fingerprint should\n",
    "        be a list of tuple(int, bool).\n",
    "        Make sure to return Row's and name the new column 'fp'.\n",
    "        No need to include raw image data in the returned RDD.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014b84e7",
   "metadata": {},
   "source": [
    "## Part 3. Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff34a772",
   "metadata": {},
   "source": [
    "### 3.1 Compute Fingerprints for the Given Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b96ff5b",
   "metadata": {},
   "source": [
    "`obj_dims` contains the dimensions for our random subspaces. Typically, these dimensions are set to the same value (which you can tune), but you can also experiment with different dimensions *at the same time* and see if it gives any better results. By default, we use eight 12-dimensional projections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0104fe7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T06:05:39.995832Z",
     "start_time": "2023-02-01T06:05:39.968128Z"
    }
   },
   "outputs": [],
   "source": [
    "# First construct a list of random linear maps\n",
    "\n",
    "# Use 8 linear projections of 12 dimensions\n",
    "obj_dims = [12 for _ in range(8)]\n",
    "\n",
    "# Fix seed for reproducibility\n",
    "np.random.seed(42)\n",
    "random_maps = get_random_linear_maps(np.prod(FP_SHAPE), obj_dims)\n",
    "\n",
    "# Next, apply the maps to all the images\n",
    "image_rdd_fp = get_rdd_with_fingerprints(image_rdd, random_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7537083f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T06:05:40.948821Z",
     "start_time": "2023-02-01T06:05:40.810905Z"
    }
   },
   "outputs": [],
   "source": [
    "_test_im = image_rdd_fp.first()\n",
    "# Check that each row of image_rdd_fp has the right attributes\n",
    "assert_equal(set(_test_im.__fields__), set(['idx', 'filename', 'label', 'fp']))\n",
    "# Check that each fingerprint has the correct length\n",
    "assert_equal(len(_test_im.fp), len(obj_dims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d83c340",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T06:05:41.023438Z",
     "start_time": "2023-02-01T06:05:41.020446Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Example figerprint: {_test_im.fp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2c4a01",
   "metadata": {},
   "source": [
    "### 3.2 Distribute Images into Buckets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648e5715",
   "metadata": {},
   "source": [
    "Here, we put all images with the same fingerprint into a single bucket for fast inference.\n",
    "\n",
    "The functions [keyBy](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.RDD.keyBy.html) and [combineByKey](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.RDD.combineByKey.html) can be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e97a5ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T06:05:42.220568Z",
     "start_time": "2023-02-01T06:05:42.176136Z"
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# This is an RDD of type RDD[(K, C)], for a “combined type” C.\n",
    "# See the documentation for combineByKey\n",
    "fp_to_bucket = ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0888a69e",
   "metadata": {},
   "source": [
    "### 3.3 Plot Bucket Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b713d5ec",
   "metadata": {},
   "source": [
    "Next, we plot the number of buckets and their sizes. Depending on the random subspaces you choose, you may see a large number of images without a match (i.e., buckets of size one), hence, we log the $y$-axis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb8088c",
   "metadata": {},
   "source": [
    "In the following cell, write code to compute the sizes of all buckets in `fp_to_bucket`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a2fb82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T06:05:56.643303Z",
     "start_time": "2023-02-01T06:05:43.196918Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# this should be a list\n",
    "bucket_sizes = ...\n",
    "\n",
    "print(f\"Found a total of {len(bucket_sizes)} buckets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f841c51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T06:05:58.935669Z",
     "start_time": "2023-02-01T06:05:58.493460Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "\n",
    "plt.hist(bucket_sizes, min(50, max(bucket_sizes)))\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Bucket size')\n",
    "plt.ylabel('Number of buckets')\n",
    "plt.show()\n",
    "\n",
    "%matplot plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e89c2a",
   "metadata": {},
   "source": [
    "### 3.4 Filter Buckets with at least Two Elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaa5c85",
   "metadata": {},
   "source": [
    "If the number of singleton buckets was too large, we can remove them to speed up operations. Write code to construct an RDD (`fp_to_bucket_filtered`) that maps a fingerprint to a bucket of size greater than 1. Also, similar to above, compute the sizes of the filtered buckets (`bucket_sizes_filtered`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c165506",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T06:06:06.875402Z",
     "start_time": "2023-02-01T06:06:00.749652Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove all the buckets that contain 1 element and return a new rdd\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "\n",
    "fp_to_bucket_filtered = ...\n",
    "\n",
    "# this should be an RDD, do not call collect()\n",
    "bucket_sizes_filtered = ...\n",
    "\n",
    "n_buckets_filtered = bucket_sizes_filtered.count()\n",
    "print(f\"Found {n_buckets_filtered} buckets with at least 2 data points.\")\n",
    "\n",
    "min_bucket_size = bucket_sizes_filtered.min()\n",
    "max_bucket_size = bucket_sizes_filtered.max()\n",
    "\n",
    "print(f\"min={min_bucket_size}, max={max_bucket_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be970972",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T06:06:09.623556Z",
     "start_time": "2023-02-01T06:06:09.620484Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check that the minimum bucket size is > 1\n",
    "assert_true(min_bucket_size > 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c12a4a",
   "metadata": {},
   "source": [
    "## Part 4. Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43085ea2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T06:06:12.002069Z",
     "start_time": "2023-02-01T06:06:11.999136Z"
    }
   },
   "outputs": [],
   "source": [
    "# We need this since we are hashing tuples which by default is not\n",
    "# a deterministic operation in Python.\n",
    "os.environ[\"PYTHONHASHSEED\"]=str(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5ed628",
   "metadata": {},
   "source": [
    "### 4.1 Implement Code to Map a Query Image to its Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f52c00",
   "metadata": {},
   "source": [
    "Here, we write two convenience functions that 1) map an image identified by its ID to a fingerprint `fp` and 2) return all the images in the bucket identified by `fp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbab599e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will be using them in the following two functions, so make\n",
    "# sure to broadcast first.\n",
    "\n",
    "image_rdd_fp_sorted_broadcast = sc.broadcast(\n",
    "    image_rdd_fp.keyBy(lambda x: x.idx).sortByKey().collectAsMap()\n",
    ")\n",
    "\n",
    "fp_to_bucket_filtered_broadcast = sc.broadcast(\n",
    "    fp_to_bucket_filtered.collectAsMap()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8c0100",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T06:06:47.827514Z",
     "start_time": "2023-02-01T06:06:12.383732Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_bucket_indices(fp):\n",
    "    \"\"\"Given a fingerprint, returns a list of indices corresponding\n",
    "    to all the elements in that bucket.\n",
    "    You can use fp_to_bucket_filtered_broadcast.\n",
    "    \n",
    "    Return an empty list if the fingerprint is not found.\n",
    "    \n",
    "    Parameters\n",
    "    __________\n",
    "    fp: tuple of tuples\n",
    "        The fingerprint.\n",
    "    \n",
    "    Returns\n",
    "    _______\n",
    "    List[int]\n",
    "        The indices of images in this bucket.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "def get_hits(idx):\n",
    "    \"\"\"Given the ID of a single image, find the bucket it has\n",
    "    been assigned to, and return a list of all indices\n",
    "    in that bucket (including the query ID).\n",
    "    You can use image_rdd_fp_sorted_broadcast.\n",
    "    \n",
    "    Parameters\n",
    "    __________\n",
    "    idx: int\n",
    "        The ID of the query image.\n",
    "    \n",
    "    Returns\n",
    "    _______\n",
    "    List[int]\n",
    "        Indices of matching images.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff9213d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-27T06:15:37.996637Z",
     "start_time": "2023-01-27T06:15:37.993846Z"
    }
   },
   "source": [
    "### 4.2 Visualize LSH Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e4a678",
   "metadata": {},
   "source": [
    "Finally, let us visualize some buckets. We provide two images in the cells below, a stegosaurus and a human face for demonstration. Feel free to test other images (you can find a full list of classes and filenames in the S3 bucket, link at the top), but make sure you **use the default two when submitting your notebook**.\n",
    "\n",
    "The first image shown will always be the query image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9f8d43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T06:06:58.010464Z",
     "start_time": "2023-02-01T06:06:50.468493Z"
    }
   },
   "outputs": [],
   "source": [
    "# sortByKey will make our lookups a lot faster.\n",
    "# source: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.lookup.html\n",
    "# Cache because we will be using this a lot.\n",
    "image_rdd_sorted = image_rdd.keyBy(lambda x: x.idx).sortByKey().cache()\n",
    "# print(image_rdd_sorted.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f94ef6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T06:11:56.625832Z",
     "start_time": "2023-02-01T06:11:56.621255Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_images(hits, cols=5):\n",
    "    plt.clf()\n",
    "    nrows = math.ceil(len(hits) / cols)\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=cols, figsize=(15, nrows*3))\n",
    "    for row, ax in zip_longest(hits, axes.flat, fillvalue=None):\n",
    "        if row is None:\n",
    "            ax.remove()\n",
    "            continue\n",
    "        ax.imshow(np.asarray(row.image_lg).reshape(SHAPE))\n",
    "        ax.set_title(f\"label={row.label}, idx={row.idx}\")\n",
    "        ax.set_axis_off()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76f6456",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T06:15:16.445961Z",
     "start_time": "2023-02-01T06:15:12.572745Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# NOTE: If you are using a subset of the data, this query\n",
    "# may not be in that set and you may get an out of range error.\n",
    "\n",
    "row1 = image_rdd.keyBy(lambda x: x.filename).lookup(\n",
    "    'stegosaurus/image_0043.jpg'\n",
    ")[0]\n",
    "test_idx1 = row1.idx\n",
    "\n",
    "hits_idx1 = get_hits(test_idx1)\n",
    "print(f\"Found {len(hits_idx1)} hits.\")\n",
    "if len(hits_idx1) == 0:\n",
    "    raise ValueError(f\"No hits were found for id {test_idx1}\")\n",
    "\n",
    "if len(hits_idx1) > 20:\n",
    "    print(\"There are too many hits; plotting only 20.\")\n",
    "    \n",
    "# move the query element to the first position\n",
    "hits_idx1.remove(test_idx1)\n",
    "hits_idx1.insert(0, test_idx1)\n",
    "\n",
    "plt.clf()\n",
    "# Take only the top 20\n",
    "hits = [image_rdd_sorted.lookup(idx)[0] for idx in islice(hits_idx1, 20)]\n",
    "plot_images(hits)  \n",
    "\n",
    "%matplot plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3265f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: If you are using a subset of the data, this query\n",
    "# may not be in that set and you may get an out of range error.\n",
    "row2 = image_rdd.keyBy(lambda x: x.filename).lookup(\n",
    "    'Faces/image_0334.jpg'\n",
    ")[0]\n",
    "test_idx2 = row2.idx\n",
    "\n",
    "hits_idx2 = get_hits(test_idx2)\n",
    "print(f\"Found {len(hits_idx2)} hits.\")\n",
    "if len(hits_idx2) == 0:\n",
    "    raise ValueError(f\"No hits were found for id {test_idx2}\")\n",
    "\n",
    "if len(hits_idx2) > 20:\n",
    "    print(\"There are too many hits; plotting only 20.\")\n",
    "\n",
    "# move the query element to the first position\n",
    "hits_idx2.remove(test_idx2)\n",
    "hits_idx2.insert(0, test_idx2)\n",
    "\n",
    "# Take only the top 20\n",
    "hits2 = [image_rdd_sorted.lookup(idx)[0] for idx in islice(hits_idx2, 20)]\n",
    "plot_images(hits2)  \n",
    "\n",
    "%matplot plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af792fb",
   "metadata": {},
   "source": [
    "## Part 5. Feature Extraction Using a Pre-Trained VGG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b6cc7b",
   "metadata": {},
   "source": [
    "Now we will use a pretrained [VGG](https://arxiv.org/abs/1409.1556) neural network to compute approximate neighbors. We remove the top classification layer from our network, leaving the last fully-connected layer, as the new output layer. The features from this last 4096-neuron fully connected layer can be used as embeddings for our images. Since the neural network was trained to predict the correct class for each image, we expect these embeddings to be similar for image belonging to the same class. We will compute exact nearest neighbors on these embeddings and comapre the neighbors we get with the LSH neighbors. Since this can be expensive to compute for all pairs of points, we only do so for the query images above.\n",
    "\n",
    "We extracted all the 4096-dimensional feature vectors for this dataset, so all we need to do is download them from the S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62266565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will download the VGG feature embeddings for\n",
    "# each image in our dataset.\n",
    "import boto3\n",
    "import boto3.session\n",
    "\n",
    "cred = boto3.Session().get_credentials()\n",
    "s3client = boto3.client('s3')\n",
    "\n",
    "response = s3client.get_object(\n",
    "    Bucket='10605-f23-hw3-data',\n",
    "    Key='caltech101_vgg_feats.pickle',\n",
    ")\n",
    "\n",
    "body = response['Body'].read()\n",
    "\n",
    "data = pickle.loads(body)\n",
    "\n",
    "# Example element\n",
    "print('Example point', data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f429f335",
   "metadata": {},
   "source": [
    "Here we make sure that each image is mapped to the right ID. Finally, all feature vectors are stacked into an array `X` of shape (n_samples, 4096)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6be14c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T07:52:59.030777Z",
     "start_time": "2023-02-01T07:52:55.734087Z"
    }
   },
   "outputs": [],
   "source": [
    "# First map each path (category, filename) to that file's idx.\n",
    "path_to_idx = image_rdd.map(\n",
    "    lambda row: (row.filename, row.idx)\n",
    ").collectAsMap()\n",
    "\n",
    "idx_to_features = {}\n",
    "for item in data:\n",
    "    # item['path'] is of the format './data/class/filename.jpg'\n",
    "    path = item['path'].split('/', maxsplit=2)[-1]\n",
    "    if path not in path_to_idx:\n",
    "        # This only happens if we are using a subset of the data\n",
    "        continue\n",
    "    idx_to_features[path_to_idx[path]] = item['features']\n",
    "\n",
    "idx_to_features = {k: idx_to_features[k] for k in sorted(idx_to_features)}\n",
    "\n",
    "X = np.vstack(list(idx_to_features.values()))\n",
    "    \n",
    "print(f\"Found {item['features'].size}-dimensional feature vectors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5a96aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T07:52:59.584483Z",
     "start_time": "2023-02-01T07:52:59.581813Z"
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(len(path_to_idx), len(idx_to_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a961b998",
   "metadata": {},
   "source": [
    "### 5.1 Compute Neighbors based on VGG Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6c8bcd",
   "metadata": {},
   "source": [
    "The following function, takes a query image ID, and computes its exact neighbors as measured by euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dcdac9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T08:01:46.134801Z",
     "start_time": "2023-02-01T08:01:46.130764Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_exact_nn_indices(query_idx, k=20):\n",
    "    \"\"\"Returns the indices of the nearest k points in X.\n",
    "    \n",
    "    Parameters\n",
    "    __________\n",
    "    query_idx: int\n",
    "        The idx of the query image.\n",
    "    k: int\n",
    "        The number of nearest neighbors to return.\n",
    "        \n",
    "    Returns\n",
    "    _______\n",
    "    List[int] or array\n",
    "        A list of integers representing the indices of the top kNN.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f692c1cf",
   "metadata": {},
   "source": [
    "### 5.2 Compare VGG Neighbors with LSH Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b63ac13",
   "metadata": {},
   "source": [
    "Which method returns neighbors of higher quality?\n",
    "\n",
    "When comparing the two methods, also take note of the runtime for both. In the LSH section, we computed fingerprints for all images and put them into buckets. In this section, we only computed exact neighbors for two query images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f69779",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T08:01:49.562919Z",
     "start_time": "2023-02-01T08:01:46.538475Z"
    }
   },
   "outputs": [],
   "source": [
    "hits_idx = get_exact_nn_indices(test_idx1)\n",
    "print(f\"Found {len(hits_idx)} hits.\")\n",
    "if len(hits_idx) > 20:\n",
    "    print(\"There are too many hits; plotting only 20.\")\n",
    "elif len(hits_idx) == 0:\n",
    "    raise ValueError(f\"No hits were found for id {idx}\")\n",
    "else:\n",
    "    # Take only the top 20\n",
    "    hits = [image_rdd_sorted.lookup(idx)[0] for idx in islice(hits_idx, 20)]\n",
    "    plot_images(hits)  \n",
    "\n",
    "%matplot plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8a6384",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T08:18:20.545377Z",
     "start_time": "2023-02-01T08:18:17.168442Z"
    }
   },
   "outputs": [],
   "source": [
    "hits_idx2 = get_exact_nn_indices(test_idx2)\n",
    "print(f\"Found {len(hits_idx2)} hits.\")\n",
    "if len(hits_idx2) > 20:\n",
    "    print(\"There are too many hits; plotting only 20.\")\n",
    "elif len(hits_idx2) == 0:\n",
    "    raise ValueError(f\"No hits were found for id {test_idx2}\")\n",
    "else:\n",
    "    # Take only the top 20\n",
    "    hits2 = [image_rdd_sorted.lookup(idx)[0] for idx in islice(hits_idx2, 20)]\n",
    "    plot_images(hits2)  \n",
    "\n",
    "%matplot plt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
